novatweets_freq_words <- findFreqTerms(novatweets_dtm_train, 5)
str(novatweets_freq_words)
# create DTMs with only the frequent terms
novatweets_dtm_freq_train <- novatweets_dtm_train[ , novatweets_freq_words]
novatweets_dtm_freq_test <- novatweets_dtm_test[ , novatweets_freq_words]
#see what is in a DTM
inspect(novatweets_dtm_freq_train)
as.character(novatweets_corpus_clean[[2360]])
as.character(novatweets_corpus[[2360]])
# convert counts to presence indicator - # times not important
convert_counts <- function(x) {
return(ifelse(x > 0, "Yes", "No"))
}
# apply() convert_counts() to columns of train/test data
novatweets_train <- apply(novatweets_dtm_freq_train, MARGIN = 2, convert_counts)
novatweets_test  <- apply(novatweets_dtm_freq_test, MARGIN = 2, convert_counts)
# Training a model on the data ----
#install.packages("e1071")
library(e1071)
novatweets_classifier <- naiveBayes(novatweets_train, novatweets_train_labels)
#Evaluating model performance ----
novatweets_test_pred <- predict(novatweets_classifier, novatweets_test)
novatweets_test_pred
library(caret)
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall")
library(wordcloud)
wordcloud(spam$tweet_txt, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$tweet_txt, max.words = 40, scale = c(3, 0.5))
table(cars.pred, cars.test$acceptability)
getwd()
setwd("D:\\Data mining and DB programming\\datasets")
# get data
fuzzycars <- read.csv2("car_data.csv")
#sample 80% rows
samp_sz = .8
#make random results reproducible
set.seed(02075049)
train = sample(1:nrow(fuzzycars), samp_sz * nrow(fuzzycars))
# test data set is other rows
cars.train = fuzzycars[train,]
cars.test = fuzzycars[-train,]
#install.packages("tree")
library(tree)
# train a model on just the training data
tree.cars.train = tree(acceptability ~ .,cars.train)
#check predictions
cars.pred = predict(tree.cars.train,cars.test,type="class")
#visually examine diffs between "ground truth" and predictions
data.frame(cars.pred, cars.test$acceptability)
table(cars.pred, cars.test$acceptability)
summary(tree.cars.train )
summary(cars.pred)
plot(tree.cars.train)
text(tree.cars.train, pretty=0)
cars.pred = predict(tree.cars.train,cars.test,type="class") # predictions
# confusion matrix and accuracy calc
library(caret)
confusionMatrix(cars.pred, cars.test$acceptability, mode = "prec_recall")
#visually examine diffs between "ground truth" and predictions
data.frame(cars.pred, cars.test$acceptability)
new_cars = data.frame(purchase_price = c("high"), maint_cost=  c("high"), num_doors = c(2), num_persons = c(4),trunk_size = c("small"),safety_rating = c("high"))
new_cars_pred = predict(tree.cars.train, new_cars,type="class")
new_cars_pred
summary(new_cars_pred)
install.packages("randomForest")
library(randomForest)
set.seed(02075049)
# build random forest and track variable importance
rf.cars=randomForest(acceptability~.,data = cars.train,importance=TRUE)
rf.cars
# show importance
importance(rf.cars)
varImpPlot(rf.cars)
# predict on test set
rf.cars.pred=predict(rf.cars,cars.test,type="class")
# confusion matrix and accuracy calc
library(caret)
confusionMatrix(rf.cars.pred, cars.test$acceptability, mode = "prec_recall")
table(cars.pred, cars.test$acceptability)
#visually examine diffs between "ground truth" and predictions
data.frame(cars.pred, cars.test$acceptability)
getwd()
setwd("D:\\Data mining and DB programming\\datasets")
# get data
novatweets <- read.csv("nova_tweets.csv")
novatweets$tod<-ifelse(novatweets$tod>6&novatweets$tod<20,"day","night")
# examine the structure of the sms data
str(novatweets)
# convert to factor.
novatweets$tod <- factor(novatweets$tod)
# examine the type variable more carefully
str(novatweets$tod)
table(novatweets$tod)
#to determine percentages of type of variable
prop.table(table(novatweets$tod))
# build a corpus (body of docs) using the text mining (tm) package
#install.packages("tm")
library(tm)
# VCorpus is an in-memory (volatile) corpus – simplest choice here
novatweets_corpus <- VCorpus(VectorSource(novatweets$tweet_txt))
# examine the sms corpus
print(novatweets_corpus)
inspect(novatweets_corpus[1:2])
# see contents
as.character(novatweets_corpus[[1]])
# returns a list in which the function has been applied to each item
lapply(novatweets_corpus[1:2], as.character)
# clean up the corpus using tm_map()
# start by making everything lowercase
novatweets_corpus_clean <- tm_map(novatweets_corpus, content_transformer(tolower))
# show the difference between sms_corpus and corpus_clean
as.character(novatweets_corpus[[1]])
as.character(novatweets_corpus_clean[[1]])
# remove numbers
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removeNumbers)
# remove stop words
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removeWords, stopwords())
# remove punctuation
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removePunctuation)
# illustration of word stemming
#install.packages("SnowballC")
library(SnowballC)
# now stem our actual corpus
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, stemDocument)
# eliminate unneeded whitespace
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, stripWhitespace)
# examine the final clean corpus
lapply(novatweets_corpus[1:3], as.character)
lapply(novatweets_corpus_clean[1:3], as.character)
# create a document-term sparse matrix
novatweets_dtm <- DocumentTermMatrix(novatweets_corpus_clean)
# see the result
novatweets_dtm
# creating training and test datasets
train_pct <- .8
set.seed(02075049)
train = sample(1:nrow(novatweets_dtm),train_pct * nrow(novatweets_dtm))
novatweets_dtm_train <- novatweets_dtm[train, ]
novatweets_dtm_test  <- novatweets_dtm[-train, ]
# also save the labels
novatweets_train_labels <- novatweets[train, ]$tod
novatweets_test_labels  <- novatweets[-train, ]$tod
# check that the proportion of spam is similar
prop.table(table(novatweets_train_labels))
prop.table(table(novatweets_test_labels))
# subset the training data into spam and ham groups
spam <- subset(novatweets, tod == "day")
# subset the training data into spam and ham groups
day <- subset(novatweets, tod == "day")
night  <- subset(novatweets, tod == "night")
# increase print output
# options(max.print = 999999)
# indicator features for frequent words
findFreqTerms(novatweets_dtm_train, 5)
# save frequently-appearing terms to a character vector
novatweets_freq_words <- findFreqTerms(novatweets_dtm_train, 5)
str(novatweets_freq_words)
# create DTMs with only the frequent terms
novatweets_dtm_freq_train <- novatweets_dtm_train[ , novatweets_freq_words]
novatweets_dtm_freq_test <- novatweets_dtm_test[ , novatweets_freq_words]
#see what is in a DTM
inspect(novatweets_dtm_freq_train)
as.character(novatweets_corpus_clean[[2360]])
as.character(novatweets_corpus[[2360]])
# convert counts to presence indicator - # times not important
convert_counts <- function(x) {
return(ifelse(x > 0, "Yes", "No"))
}
# apply() convert_counts() to columns of train/test data
novatweets_train <- apply(novatweets_dtm_freq_train, MARGIN = 2, convert_counts)
novatweets_test  <- apply(novatweets_dtm_freq_test, MARGIN = 2, convert_counts)
# Training a model on the data ----
#install.packages("e1071")
library(e1071)
novatweets_classifier <- naiveBayes(novatweets_train, novatweets_train_labels)
#Evaluating model performance ----
novatweets_test_pred <- predict(novatweets_classifier, novatweets_test)
novatweets_test_pred
library(caret)
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall")
getwd()
setwd("D:\\Data mining and DB programming\\datasets")
# get data
novatweets <- read.csv("nova_tweets.csv")
novatweets$tod<-ifelse(novatweets$tod>6&novatweets$tod<20,"day","night")
# examine the structure of the sms data
str(novatweets)
# convert to factor.
novatweets$tod <- factor(novatweets$tod)
# examine the type variable more carefully
str(novatweets$tod)
table(novatweets$tod)
#to determine percentages of type of variable
prop.table(table(novatweets$tod))
# build a corpus (body of docs) using the text mining (tm) package
#install.packages("tm")
library(tm)
# VCorpus is an in-memory (volatile) corpus – simplest choice here
novatweets_corpus <- VCorpus(VectorSource(novatweets$tweet_txt))
# examine the sms corpus
print(novatweets_corpus)
inspect(novatweets_corpus[1:2])
# see contents
as.character(novatweets_corpus[[1]])
# returns a list in which the function has been applied to each item
lapply(novatweets_corpus[1:2], as.character)
# clean up the corpus using tm_map()
# start by making everything lowercase
novatweets_corpus_clean <- tm_map(novatweets_corpus, content_transformer(tolower))
# show the difference between sms_corpus and corpus_clean
as.character(novatweets_corpus[[1]])
as.character(novatweets_corpus_clean[[1]])
# remove numbers
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removeNumbers)
# remove stop words
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removeWords, stopwords())
# remove punctuation
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removePunctuation)
# illustration of word stemming
#install.packages("SnowballC")
library(SnowballC)
# now stem our actual corpus
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, stemDocument)
# eliminate unneeded whitespace
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, stripWhitespace)
# examine the final clean corpus
lapply(novatweets_corpus[1:3], as.character)
lapply(novatweets_corpus_clean[1:3], as.character)
# create a document-term sparse matrix
novatweets_dtm <- DocumentTermMatrix(novatweets_corpus_clean)
# see the result
novatweets_dtm
# creating training and test datasets
train_pct <- .8
set.seed(02075049)
train = sample(1:nrow(novatweets_dtm),train_pct * nrow(novatweets_dtm))
novatweets_dtm_train <- novatweets_dtm[train, ]
novatweets_dtm_test  <- novatweets_dtm[-train, ]
# also save the labels
novatweets_train_labels <- novatweets[train, ]$tod
novatweets_test_labels  <- novatweets[-train, ]$tod
# check that the proportion of spam is similar
prop.table(table(novatweets_train_labels))
prop.table(table(novatweets_test_labels))
# subset the training data into spam and ham groups
day <- subset(novatweets, tod == "day")
night  <- subset(novatweets, tod == "night")
# word cloud visualization
#install.packages("wordcloud")
library(wordcloud)
wordcloud(spam$tweet_txt, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$tweet_txt, max.words = 40, scale = c(3, 0.5))
# increase print output
# options(max.print = 999999)
# indicator features for frequent words
findFreqTerms(novatweets_dtm_train, 5)
# save frequently-appearing terms to a character vector
novatweets_freq_words <- findFreqTerms(novatweets_dtm_train, 5)
str(novatweets_freq_words)
# create DTMs with only the frequent terms
novatweets_dtm_freq_train <- novatweets_dtm_train[ , novatweets_freq_words]
novatweets_dtm_freq_test <- novatweets_dtm_test[ , novatweets_freq_words]
#see what is in a DTM
inspect(novatweets_dtm_freq_train)
as.character(novatweets_corpus_clean[[2360]])
as.character(novatweets_corpus[[2360]])
# convert counts to presence indicator - # times not important
convert_counts <- function(x) {
return(ifelse(x > 0, "Yes", "No"))
}
# apply() convert_counts() to columns of train/test data
novatweets_train <- apply(novatweets_dtm_freq_train, MARGIN = 2, convert_counts)
novatweets_test  <- apply(novatweets_dtm_freq_test, MARGIN = 2, convert_counts)
# Training a model on the data ----
#install.packages("e1071")
library(e1071)
novatweets_classifier <- naiveBayes(novatweets_train, novatweets_train_labels)
#Evaluating model performance ----
novatweets_test_pred <- predict(novatweets_classifier, novatweets_test)
novatweets_test_pred
library(caret)
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall")
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall", positive = "day" )
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall", positive = "night" )
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall", positive = "day" )
# read in data and examine structure
concrete <- read.csv("concrete.csv")
str(concrete)
View(concrete)
# apply normalization to entire data frame
# NN will work best when data normalized in range
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
View(concrete_norm)
# confirm that the range is now between zero and one
summary(concrete$strength)
summary(concrete_norm$strength)
# create training and test data
train_pct <- 0.75
set.seed(123)
train <- sample(1:nrow(concrete_norm),train_pct * nrow(concrete_norm))
concrete_train <- concrete_norm[train, ]
concrete_test <- concrete_norm[-train, ]
install.packages("neuralnet")
#install.packages("neuralnet")
library(neuralnet)
# simple ANN with only a single hidden neuron
net <- neuralnet(strength~.,concrete_train,hidden=1)
plot(net)
# obtain model results
net_results <- compute(net, concrete_test[,1:8])
# obtain predicted strength values
predicted_strength <- net_results$net.result
# examine the correlation between predicted and actual values
cor(predicted_strength, concrete_test$strength)
# a more complex neural network topology with 5 hidden neurons
net2 <- neuralnet(strength~.,concrete_train,hidden=5)
plot(net2)
plot(net)
# obtain model results
net_results2 <- compute(net2, concrete_test[,1:8])
# obtain predicted strength values
predicted_strength2 <- net_results2$net.result
# examine the correlation between predicted and actual values
cor(predicted_strength2, concrete_test$strength)
lm.fit = lm(strength~.,concrete_train)
summary(lm.fit)
lm_preds = predict(lm.fit,concrete_test[,1:8])
cor(lm_preds, concrete_test$strength)
View(novatweets)
View(novatweets_test)
View(novatweets_test)
View(novatweets_test)
getwd()
setwd("D:\\Data mining and DB programming\\datasets")
# get data
novatweets <- read.csv("nova_tweets.csv")
novatweets$tod<-ifelse(novatweets$tod>6&novatweets$tod<20,"day","night")
# examine the structure of the sms data
str(novatweets)
# convert to factor.
novatweets$tod <- factor(novatweets$tod)
# examine the type variable more carefully
str(novatweets$tod)
table(novatweets$tod)
#to determine percentages of type of variable
prop.table(table(novatweets$tod))
# build a corpus (body of docs) using the text mining (tm) package
#install.packages("tm")
library(tm)
# VCorpus is an in-memory (volatile) corpus – simplest choice here
novatweets_corpus <- VCorpus(VectorSource(novatweets$tweet_txt))
# examine the sms corpus
print(novatweets_corpus)
inspect(novatweets_corpus[1:2])
# see contents
as.character(novatweets_corpus[[1]])
# returns a list in which the function has been applied to each item
lapply(novatweets_corpus[1:2], as.character)
# clean up the corpus using tm_map()
# start by making everything lowercase
novatweets_corpus_clean <- tm_map(novatweets_corpus, content_transformer(tolower))
# show the difference between sms_corpus and corpus_clean
as.character(novatweets_corpus[[1]])
as.character(novatweets_corpus_clean[[1]])
# remove numbers
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removeNumbers)
# remove stop words
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removeWords, stopwords())
# remove punctuation
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, removePunctuation)
# illustration of word stemming
#install.packages("SnowballC")
library(SnowballC)
# now stem our actual corpus
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, stemDocument)
# eliminate unneeded whitespace
novatweets_corpus_clean <- tm_map(novatweets_corpus_clean, stripWhitespace)
# examine the final clean corpus
lapply(novatweets_corpus[1:3], as.character)
lapply(novatweets_corpus_clean[1:3], as.character)
# create a document-term sparse matrix
novatweets_dtm <- DocumentTermMatrix(novatweets_corpus_clean)
# see the result
novatweets_dtm
# creating training and test datasets
train_pct <- .8
set.seed(02075049)
train = sample(1:nrow(novatweets_dtm),train_pct * nrow(novatweets_dtm))
novatweets_dtm_train <- novatweets_dtm[train, ]
novatweets_dtm_test  <- novatweets_dtm[-train, ]
# also save the labels
novatweets_train_labels <- novatweets[train, ]$tod
novatweets_test_labels  <- novatweets[-train, ]$tod
# check that the proportion of spam is similar
prop.table(table(novatweets_train_labels))
prop.table(table(novatweets_test_labels))
# subset the training data into spam and ham groups
day <- subset(novatweets, tod == "day")
night  <- subset(novatweets, tod == "night")
# word cloud visualization
#install.packages("wordcloud")
library(wordcloud)
wordcloud(spam$tweet_txt, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$tweet_txt, max.words = 40, scale = c(3, 0.5))
# increase print output
# options(max.print = 999999)
# indicator features for frequent words
findFreqTerms(novatweets_dtm_train, 5)
# save frequently-appearing terms to a character vector
novatweets_freq_words <- findFreqTerms(novatweets_dtm_train, 5)
str(novatweets_freq_words)
# create DTMs with only the frequent terms
novatweets_dtm_freq_train <- novatweets_dtm_train[ , novatweets_freq_words]
novatweets_dtm_freq_test <- novatweets_dtm_test[ , novatweets_freq_words]
#see what is in a DTM
inspect(novatweets_dtm_freq_train)
as.character(novatweets_corpus_clean[[2360]])
as.character(novatweets_corpus[[2360]])
# convert counts to presence indicator - # times not important
convert_counts <- function(x) {
return(ifelse(x > 0, "Yes", "No"))
}
# apply() convert_counts() to columns of train/test data
novatweets_train <- apply(novatweets_dtm_freq_train, MARGIN = 2, convert_counts)
novatweets_test  <- apply(novatweets_dtm_freq_test, MARGIN = 2, convert_counts)
# Training a model on the data ----
#install.packages("e1071")
library(e1071)
novatweets_classifier <- naiveBayes(novatweets_train, novatweets_train_labels)
#Evaluating model performance ----
novatweets_test_pred <- predict(novatweets_classifier, novatweets_test)
novatweets_test_pred
library(caret)
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall", positive = "day" )
#to determine percentages of type of variable
prop.table(table(novatweets$tod))
confusionMatrix(novatweets_test_pred, novatweets_test_labels, mode = "prec_recall", positive = "night" )
setwd("D:\\Data mining and DB programming\\datasets")
# get data
student_alcohol <- read.csv("student_alcohol.csv")
# get data
student_alcohol <- read.csv("student_alcohol.csv")
View(student_alcohol)
posts<-subset(df,select=-c(famsize, Pstatus, Medu, Fedu, Mjob,  Fjob, reason, guardian, traveltime,
studytime, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic,health, studentid))
posts<-subset(df,select=-c(famsize, Pstatus, Medu, Fedu, Mjob,  Fjob, reason, guardian, traveltime,
studytime, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic,health, studentid))
posts<-subset(student_alcohol,select=-c(famsize, Pstatus, Medu, Fedu, Mjob,  Fjob, reason, guardian, traveltime,
studytime, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic,health, studentid))
# get data
student_alcohol_raw <- read.csv("student_alcohol.csv")
student_alcohol<-subset(student_alcohol_raw,select=-c(famsize, Pstatus, Medu, Fedu, Mjob,  Fjob, reason, guardian, traveltime,
studytime, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic,health, studentid))
View(student_alcohol)
View(student_alcohol_raw)
summary(student_alcohol)
#create a linear model
lm.all_predictors <- lm( Walc ~ ., data = student_alcohol)
summary(lm.all_predictors)
# creating training and test datasets
train_pct <- .8
set.seed(02075049)
train = sample(1:nrow(student_alcohol),train_pct * nrow(student_alcohol))
student_alcohol_train <- student_alcohol[train, ]
student_alcohol_test  <- student_alcohol[-train, ]
#create a linear model with training data
lm.all_predictors_train <- lm( Walc ~ ., data = student_alcohol_train)
summary(lm.all_predictors_train)
pred1<-predict(lm.all_predictors_train, data.frame(student_alcohol_test))
table(pred1,student_alcohol_test$Walc)
summary(pred1)
pred2 <- predict(lm.all_predictors_train, data.frame(student_alcohol_predict))
pred1
summary(student_alcohol_train$Walc)
summary(pred2)
summary(pred1)
train_pct <- .8
set.seed(02075049)
train = sample(1:nrow(student_alcohol),train_pct * nrow(student_alcohol))
student_alcohol_train1 <- student_alcohol[train, ]
student_alcohol_test1  <- student_alcohol[-train, ]
# train a model on just the training data
tree.all_predictors = tree(Walc ~ .,student_alcohol_train1)
lm.pred<-predict(lm.all_predictors_train, data.frame(student_alcohol_test))
#check predictions
tree.pred = predict(tree.all_predictors, student_alcohol_test1, type="class")
#check predictions
tree.pred = predict(tree.all_predictors, student_alcohol_test1)
table(tree.pred, student_alcohol_test1$Walc)
summary(tree.pred)
tree.pred
plot(tree.all_predictors)
text(tree.all_predictors, pretty=0
text(tree.all_predictors, pretty=0)
plot(tree.all_predictors)
text(tree.all_predictors, pretty=0)
confusionMatrix(tree.pred, student_alcohol_test1$Walc, mode = "prec_recall”, positive="Yes")
confusionMatrix(tree.pred, student_alcohol_test1$Walc, mode = "prec_recall”)
# confusion matrix and accuracy calc
library(caret)
confusionMatrix(tree.pred, student_alcohol_test1$Walc, mode = "prec_recall”)
confusionMatrix(tree.pred, student_alcohol_test1$Walc, mode = "prec_recall")
# confusion matrix and accuracy calc
library(caret)
confusionMatrix(lm.pred, student_alcohol_test$Walc, mode = "prec_recall")
# get data
fuzzycars <- read.csv2("car_data.csv")
View(fuzzycars)
